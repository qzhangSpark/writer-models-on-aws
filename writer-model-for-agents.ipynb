{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2587fe-b55b-4f00-ad70-3c4633797a8e",
   "metadata": {},
   "source": [
    "# Build AI agents with Writer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c432578-3050-424f-8ce3-4c65c9e88f66",
   "metadata": {},
   "source": [
    "Writer models support advanced tool-calling capabilities, enabling complex multi-step workflows and agentic actions. This notebook shows you how to use Writer models and LlamaIndex to build an AI agent on AWS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee52ec9-2ab8-4152-8818-24cb6a5540f8",
   "metadata": {},
   "source": [
    "## Install and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fc9c-8b43-41ff-9f4d-370a474cc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U llama-index\n",
    "!pip install llama-index-llms-bedrock-converse\n",
    "!pip install llama_index-tools-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f5c981a-6142-45ba-a644-6b195a6dfd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate, Settings\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.memory.types import ChatMessage\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.tools.arxiv.base import ArxivToolSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200caad6-afe5-476b-a872-51231d16c290",
   "metadata": {},
   "source": [
    "## Define functions as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "489df22c-6449-46cb-9777-9584819003a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def github_search(topic: str, num_results: int = 3, sort_by: str = \"stars\") -> list:\n",
    "    \"\"\"Search GitHub repositories by topic\"\"\"\n",
    "    url = f\"https://api.github.com/search/repositories?q=topic:{topic}&sort={sort_by}&order=desc\"\n",
    "    response = requests.get(url).json()\n",
    "    code_repos = [\n",
    "        {\n",
    "            'html_url': item['html_url'],\n",
    "            'description': item['description'],\n",
    "            'stargazers_count': item['stargazers_count'],\n",
    "        }\n",
    "        for item in response['items'][:num_results]\n",
    "    ]\n",
    "    return code_repos\n",
    "\n",
    "def news_search(topic: str, num_results: int = 3) -> list:\n",
    "    \"\"\"Search for news using NewsAPI\"\"\"\n",
    "    API_KEY = 'f2a5f3027b2c46bd9d313c7c39b8ea06'\n",
    "    BASE_URL = 'https://newsapi.org/v2/everything'\n",
    "    \n",
    "    try:\n",
    "        params = {\n",
    "            'q': topic,\n",
    "            'apiKey': API_KEY,\n",
    "            'sortBy': 'publishedAt',\n",
    "            'language': 'en',\n",
    "            'domains': 'techcrunch.com,wired.com,theverge.com,venturebeat.com,arstechnica.com,deeplearning.ai',\n",
    "            'pageSize': num_results\n",
    "        }\n",
    "        \n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data['status'] == 'ok' and data['totalResults'] > 0:\n",
    "                articles = []\n",
    "                for article in data['articles'][:num_results]:\n",
    "                    articles.append({\n",
    "                        'title': article['title'],\n",
    "                        'link': article['url'],\n",
    "                        'published': article['publishedAt'],\n",
    "                        'summary': article['description'],\n",
    "                        'content': article['content']\n",
    "                    })\n",
    "                return articles\n",
    "            else:\n",
    "                print(f\"No results found for topic: {topic}\")\n",
    "                return []\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limit exceeded\")\n",
    "            return []\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee980dc6-c542-4dba-b6fd-c6a5c64056bc",
   "metadata": {},
   "source": [
    "## Create tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1946d5-0a0a-4d88-a941-2eb6f9096140",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_tool = FunctionTool.from_defaults(fn=github_search)\n",
    "news_tool = FunctionTool.from_defaults(fn=news_search)\n",
    "arxiv_tool = ArxivToolSpec() # Connect to ArXiv and search for recent papers \n",
    "\n",
    "api_tools = arxiv_tool.to_tool_list()\n",
    "api_tools.extend([news_tool, github_tool])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770125c-b6a2-4431-a049-e8e0390227f4",
   "metadata": {},
   "source": [
    "## Select the Writer model as LLM for the AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c5320d2-0625-40e9-9921-51e82ac5afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"us.writer.palmyra-x5-v1:0\"\n",
    "# model_id = \"arn:aws:bedrock:us-west-2:YOUR AWS ACCOUNT ID:inference-profile/us.writer.palmyra-x5-v1:0\"\n",
    "\n",
    "llm = BedrockConverse(model=model_id, max_tokens=4000)\n",
    "Settings.llm = BedrockConverse(model=model_id, max_tokens=4000)\n",
    "\n",
    "# Create memory buffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b31b3-8144-49a0-8b2c-a4ffd45d1732",
   "metadata": {},
   "source": [
    "## Create the AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd50ca0c-b950-4647-bcaf-19bd135ed05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt \n",
    "system_prompt = \"\"\"\n",
    "You are a friendly and engaging technology expert with access to the GitHub API, arXiv API, and TechCrunch API. \n",
    "Think of yourself as an enthusiastic tech researcher having a conversation with a colleague.\n",
    "Your news_search tool returns the absolute latest articles, often just minutes or hours old.\n",
    "## Tools\n",
    "You have access to these tools and are responsible for using them in any sequence you deem appropriate:\n",
    "{tool_desc}\n",
    "\n",
    "## Output Format\n",
    "To answer the question, please use the following format:\n",
    "\n",
    "Thought: Explain your thinking process conversationally\n",
    "Action: tool name (one of {tool_names})\n",
    "Action Input: the input to the tool in JSON format\n",
    "\n",
    "If a tool returns no results, try:\n",
    "1. Using a different tool\n",
    "2. Modifying your search terms\n",
    "3. If all tools fail, acknowledge that you couldn't find the information\n",
    "\n",
    "When you get an empty result, don't repeat the same exact search - try a different approach.\n",
    "\n",
    "Continue until you can answer or determine you can't find the information, then use:\n",
    "\n",
    "Thought: [Explain your conclusion]\n",
    "Answer: [Provide your findings or explain why you couldn't find the information]\n",
    "\n",
    "## Additional Rules\n",
    "- If a search returns no results, try alternative search terms or different tools\n",
    "- Don't repeat the same search multiple times\n",
    "- If no information is found after trying different approaches, acknowledge the limitation\n",
    "- Use a conversational, engaging tone\n",
    "- Connect ideas across different sources when relevant\n",
    "- Highlight what's exciting or surprising\n",
    "- Keep it concise but interesting\n",
    "- Always check the dates of the articles and mention if they're not recent\n",
    "- If articles are more than 30 days old, try another search or tool\n",
    "- Format dates in a user-friendly way (e.g., \"2 days ago\" or \"March 15, 2024\")\n",
    "- Keep responses clear and concise, but when the answer should be longer format feel free to make it so if it helps the user understand the full answer.\n",
    "\n",
    "## Current Conversation\n",
    "Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "\"\"\"\n",
    "\n",
    "# Create and configure the agent\n",
    "react_system_prompt = PromptTemplate(template=system_prompt)\n",
    "agent = ReActAgent.from_tools(\n",
    "    api_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=10,\n",
    "    memory=memory\n",
    ")\n",
    "agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt})\n",
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83813060-b684-4631-b0ad-53d87d5dd103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step b48500af-0945-4499-8b4b-02e6f8b8e423. Step input: find a paper on Model Context Protocol and summarize the paper\n",
      "\u001b[1;3;38;5;200mThought: To find a paper on Model Context Protocol, I'll start by querying arXiv.\n",
      "Action: arxiv_query\n",
      "Action Input: {'query': 'Model Context Protocol', 'sort_by': 'relevance'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: [Document(id_='d5bbcb73-7541-4c06-a54d-1d359f907003', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='http://arxiv.org/pdf/2505.02279v2: A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)\\nLarge language model powered autonomous agents demand robust, standardized\\nprotocols to integrate tools, share contextual data, and coordinate tasks\\nacross heterogeneous systems. Ad-hoc integrations are difficult to scale,\\nsecure, and generalize across domains. This survey examines four emerging agent\\ncommunication protocols: Model Context Protocol (MCP), Agent Communication\\nProtocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol\\n(ANP), each addressing interoperability in deployment contexts. MCP provides a\\nJSON-RPC client-server interface for secure tool invocation and typed data\\nexchange. ACP defines a general-purpose communication protocol over RESTful\\nHTTP, supporting MIME-typed multipart messages and synchronous and asynchronous\\ninteractions. Its lightweight and runtime-independent design enables scalable\\nagent invocation, while features like session management, message routing, and\\nintegration with role-based and decentralized identifiers (DIDs). A2A enables\\npeer-to-peer task delegation using capability-based Agent Cards, supporting\\nsecure and scalable collaboration across enterprise agent workflows. ANP\\nsupports open network agent discovery and secure collaboration using W3C\\ndecentralized identifiers DIDs and JSON-LD graphs. The protocols are compared\\nacross multiple dimensions, including interaction modes, discovery mechanisms,\\ncommunication patterns, and security models. Based on the comparative analysis,\\na phased adoption roadmap is proposed: beginning with MCP for tool access,\\nfollowed by ACP for structured, multimodal messaging session-aware interaction\\nand both online and offline agent discovery across scalable, HTTP-based\\ndeployments A2A for collaborative task execution, and extending to ANP for\\ndecentralized agent marketplaces. This work provides a comprehensive foundation\\nfor designing secure, interoperable, and scalable ecosystems of LLM-powered\\nagents.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1d1d248b-61ad-4ed8-beb9-749d17502322', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='http://arxiv.org/pdf/1509.03646v1: Key Exchange Protocol in the Trusted Data Servers Context\\nThe aim of this technical report is to complement the work in [To et al.\\n2014] by proposing a Group Key Exchange protocol so that the Querier and TDSs\\n(and TDSs themselves) can securely create and exchange the shared key. Then,\\nthe security of this protocol is formally proved using the game-based model.\\nFinally, we perform the comparison between this protocol and other related\\nworks.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9a1c0aac-f21f-48c3-8998-c4b2e35a77df', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='http://arxiv.org/pdf/2103.16325v1: Systematic Mapping Protocol: Reasoning Algorithms on Feature Model\\nContext: The importance of the feature modeling for the software product\\nlines considering the modeling and management of the variability. Objective:\\nDefine a protocol to conduct a systematic mapping study to summarize and\\nsynthesize the evidence on reasoning algorithms for feature modeling. Method:\\nApplication the protocol to conduct a systematic mapping study according the\\nguidelines of K. Petersen. Results: A validated protocol to conduct a\\nsystematic mapping study. Conclusions: Initial findings show that a more\\ndetailed review for the different reasoning algorithms for feature modeling is\\nneeded.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "\u001b[0m> Running step d3401ad3-7b58-4251-9670-ed6d3f378eb6. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The search results provide a relevant paper on Model Context Protocol (MCP). I'll summarize the findings from the most relevant document.\n",
      "\n",
      "The paper discusses four emerging agent communication protocols, with a focus on Model Context Protocol (MCP). Here's a summary:\n",
      "\n",
      "* MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange.\n",
      "* It's designed to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems.\n",
      "* The protocol is compared with other emerging protocols like Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP) across multiple dimensions.\n",
      "* A phased adoption roadmap is proposed, starting with MCP for tool access, followed by ACP for structured messaging, A2A for collaborative task execution, and ANP for decentralized agent marketplaces.\n",
      "\n",
      "The paper provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.\n",
      "\n",
      "To get more information or check if there are more recent updates, I'll perform a news search.\n",
      "Action: news_search\n",
      "Action Input: {'topic': 'Model Context Protocol', 'num_results': 3}\n",
      "\u001b[0mError fetching news: name 'requests' is not defined\n",
      "\u001b[1;3;34mObservation: []\n",
      "\u001b[0m> Running step 56d41c3b-c1dd-4e19-a376-ef38b6732d10. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The news search didn't return any results. I'll try a GitHub search to see if there are any repositories related to Model Context Protocol.\n",
      "Action: github_search\n",
      "Action Input: {'topic': 'Model Context Protocol', 'num_results': 3, 'sort_by': 'stars'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: name 'requests' is not defined\n",
      "\u001b[0m> Running step 9455947f-943b-46d3-ba3e-1beff3d981fb. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems there's an issue with the GitHub search tool. I'll try to directly access the information from the arXiv paper I found earlier.\n",
      "\n",
      "The paper on arXiv discusses the Model Context Protocol (MCP) and its role in agent communication. Since I couldn't find any recent news or GitHub repositories, the arXiv paper remains the most relevant source of information.\n",
      "\n",
      "Thought: I'll provide a summary based on the arXiv paper.\n",
      "Answer: The Model Context Protocol (MCP) is a JSON-RPC client-server interface designed for secure tool invocation and typed data exchange. It's part of a set of emerging protocols aimed at enabling interoperability among LLM-powered agents. MCP is proposed as a starting point for tool access in a phased adoption roadmap for designing secure, interoperable, and scalable agent ecosystems.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"find a paper on Model Context Protocol and summarize the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b780fcf-ac6e-4e19-80cb-67914230904b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 1cca2c4b-3e3b-4e73-a33f-b12dbd44b6ab. Step input: find github sample codes for writer's palmyra-x5 model\n",
      "\u001b[1;3;38;5;200mThought: To find GitHub sample codes for the Writer's Palmyra-X5 model, I'll start by searching for the model on GitHub.\n",
      "Action: github_search\n",
      "Action Input: {'topic': 'Palmyra-X5 Writer', 'num_results': 3}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: name 'requests' is not defined\n",
      "\u001b[0m> Running step 9b788830-ad5e-4c2f-ad8a-ac42a76762b8. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems like there's an issue with the GitHub search tool. Let's try to search for the Palmyra-X5 model on arXiv to see if there are any related papers or references to GitHub repositories.\n",
      "Action: arxiv_query\n",
      "Action Input: {'query': 'Palmyra-X5 Writer model', 'sort_by': 'relevance'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: [Document(id_='5cc285da-8d9e-432a-ac0d-6eed4247a3b1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='http://arxiv.org/pdf/1606.06472v2: DeepWriter: A Multi-Stream Deep CNN for Text-independent Writer Identification\\nText-independent writer identification is challenging due to the huge\\nvariation of written contents and the ambiguous written styles of different\\nwriters. This paper proposes DeepWriter, a deep multi-stream CNN to learn deep\\npowerful representation for recognizing writers. DeepWriter takes local\\nhandwritten patches as input and is trained with softmax classification loss.\\nThe main contributions are: 1) we design and optimize multi-stream structure\\nfor writer identification task; 2) we introduce data augmentation learning to\\nenhance the performance of DeepWriter; 3) we introduce a patch scanning\\nstrategy to handle text image with different lengths. In addition, we find that\\ndifferent languages such as English and Chinese may share common features for\\nwriter identification, and joint training can yield better performance.\\nExperimental results on IAM and HWDB datasets show that our models achieve high\\nidentification accuracy: 99.01% on 301 writers and 97.03% on 657 writers with\\none English sentence input, 93.85% on 300 writers with one Chinese character\\ninput, which outperform previous methods with a large margin. Moreover, our\\nmodels obtain accuracy of 98.01% on 301 writers with only 4 English alphabets\\nas input.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b9e125ad-17b0-4024-b9cf-65de71a9a0ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='http://arxiv.org/pdf/1707.06828v2: HMM-based Writer Identification in Music Score Documents without Staff-Line Removal\\nWriter identification from musical score documents is a challenging task due\\nto its inherent problem of overlapping of musical symbols with staff lines.\\nMost of the existing works in the literature of writer identification in\\nmusical score documents were performed after a preprocessing stage of staff\\nlines removal. In this paper we propose a novel writer identification framework\\nin musical documents without removing staff lines from documents. In our\\napproach, Hidden Markov Model has been used to model the writing style of the\\nwriters without removing staff lines. The sliding window features are extracted\\nfrom musical score lines and they are used to build writer specific HMM models.\\nGiven a query musical sheet, writer specific confidence for each musical line\\nis returned by each writer specific model using a loglikelihood score. Next, a\\nloglikelihood score in page level is computed by weighted combination of these\\nscores from the corresponding line images of the page. A novel Factor Analysis\\nbased feature selection technique is applied in sliding window features to\\nreduce the noise appearing from staff lines which proves efficiency in writer\\nidentification performance.In our framework we have also proposed a novel score\\nline detection approach in musical sheet using HMM. The experiment has been\\nperformed in CVC-MUSCIMA dataset and the results obtained that the proposed\\napproach is efficient for score line detection and writer identification\\nwithout removing staff lines. To get the idea of computation time of our\\nmethod, detail analysis of execution time is also provided.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4c0c9c36-f7c3-4fcf-8be3-d220ab568075', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"http://arxiv.org/pdf/2309.07727v1: PerPLM: Personalized Fine-tuning of Pretrained Language Models via Writer-specific Intermediate Learning and Prompts\\nThe meanings of words and phrases depend not only on where they are used\\n(contexts) but also on who use them (writers). Pretrained language models\\n(PLMs) are powerful tools for capturing context, but they are typically\\npretrained and fine-tuned for universal use across different writers. This\\nstudy aims to improve the accuracy of text understanding tasks by personalizing\\nthe fine-tuning of PLMs for specific writers. We focus on a general setting\\nwhere only the plain text from target writers are available for\\npersonalization. To avoid the cost of fine-tuning and storing multiple copies\\nof PLMs for different users, we exhaustively explore using writer-specific\\nprompts to personalize a unified PLM. Since the design and evaluation of these\\nprompts is an underdeveloped area, we introduce and compare different types of\\nprompts that are possible in our setting. To maximize the potential of\\nprompt-based personalized fine-tuning, we propose a personalized intermediate\\nlearning based on masked language modeling to extract task-independent traits\\nof writers' text. Our experiments, using multiple tasks, datasets, and PLMs,\\nreveal the nature of different prompts and the effectiveness of our\\nintermediate learning approach.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "\u001b[0m> Running step 2032a43f-1461-4ca0-a24f-2523619176ee. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: The search results from arXiv don't directly relate to the Palmyra-X5 model. Let's try a different approach by searching for news or articles related to the Writer's Palmyra-X5 model.\n",
      "Action: news_search\n",
      "Action Input: {'topic': 'Writer Palmyra-X5 model', 'num_results': 3}\n",
      "\u001b[0mError fetching news: name 'requests' is not defined\n",
      "\u001b[1;3;34mObservation: []\n",
      "\u001b[0m> Running step ed36de72-c0d9-495c-b981-f814f0afbd05. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: Since the news search didn't yield any results, I'll try to refine my GitHub search query to find sample codes for the Writer's Palmyra-X5 model.\n",
      "Action: github_search\n",
      "Action Input: {'topic': 'Palmyra-X5', 'num_results': 3}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: name 'requests' is not defined\n",
      "\u001b[0m> Running step 18f4a26c-a25a-4a67-aafb-3163db6502cd. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: It seems like the GitHub search tool is not functioning properly due to a technical issue. Let's try to find information about the Palmyra-X5 model through a general web search or by looking for related models or technologies.\n",
      "\n",
      "Since I'm unable to perform a successful search with the available tools, I'll provide a summary of the situation.\n",
      "Answer: I was unable to find sample codes for the Writer's Palmyra-X5 model on GitHub due to technical issues with the search tool. I also couldn't find any relevant papers on arXiv or news articles related to the model. If you have more context or details about the Palmyra-X5 model, I may be able to help you better.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"find github sample codes for writer's palmyra-x5 model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94965034-3ce8-4a6b-b0e7-0089715da357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
